


%pip install pinecone-client


import pandas as pd;
import numpy as np;
import matplotlib.pyplot as plt;
import seaborn as sns;
import os;
import time;
import itertools
from pinecone import Pinecone, ServerlessSpec, PodSpec;
from collections import Counter









use_serverless = False






# initialize connection to pinecone 
api_key = os.environ.get('PINECONE_API') 
environment = os.environ.get('PINECONE_ENVIRONMENT')

# configure client
pc = Pinecone(api_key = api_key)

#we setup our index specification
if use_serverless:
    spec = ServerlessSpec(cloud='aws', region='us-west-2')
else:
    spec = PodSpec(environment='gcp-starter')





index_name = 'hello-pinecone'

def create_index(index_name):
    # ensure no duplicates
    if index_name in pc.list_indexes().names():
        pc.delete_index(index_name)

    dimensions = 3
    pc.create_index(
        name = index_name, 
        dimension = dimensions, # vector dim
        metric = 'cosine', # common similarity
        spec = spec
    )

    # wait for index to be ready before connecting
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

create_index(index_name)


index = pc.Index(index_name)





df = pd.DataFrame(
    data={
        "id": ["A", "B"],
        "vector": [[1., 1., 1.], [1., 2., 3.]]
    })
df


index.upsert(vectors=zip(df.id, df.vector))  # insert vectors





pc.delete_index(index_name)








create_index(index_name)
index = pc.Index(index_name)


df = pd.DataFrame()
df["id"] = ["A", "B", "C", "D", "E"]
df["vector"] = [[1., 1.,1.], [2., 2.,2.], [3., 3.,3.], [4., 4.,4.], [5., 5.,5.]]
df


index.upsert(vectors=zip(df.id, df.vector))





fetch_results = index.fetch(ids  = ["A", "B","C"])
fetch_results





index.upsert(vectors = [("A", [0.1,0.1,0.1])])


fetch_result = index.fetch(ids=["A"])
fetch_result





index.describe_index_stats()


pc.delete_index(index_name)








create_index(index_name)
index = pc.Index(index_name)

df = pd.DataFrame()
df["id"] = ["F-1", "F-2", "S-1", "S-2"]
df["vector"] = [[1., 1.,1.], [2., 2., 2.], [3., 3., 3.], [4., 4., 4.]]
df["metadata"] = [
    {"category": "finance", "published": 2015},
    {"category": "finance", "published": 2016},
    {"category": "sport", "published": 2017},
    {"category": "sport", "published": 2018}]
df


# Insert vectors without specifying a namespace
index.upsert(vectors=zip(df.id, df.vector, df.metadata))
index.describe_index_stats()

# fetch the vectors
index.fetch(ids=["F-1"])





query_results = index.query(vector=df[df.id == "F-1"].vector[0], top_k=3)
query_results


filter_condition = {
    "category" : {"$eq": "finance"},
    "published": {"$gt": 2015 }
}
query_results = index.query(vector=
    df[df.id == "F-1"].vector[0], top_k=3, filter=filter_condition
)
query_results


pc.delete_index(index_name)





create_index(index_name)
index = pc.Index(index_name)

df = pd.DataFrame()
df["id"] = ["Wall-E", "Up", "Ratatouille", "Toy Story", "Star wars"]
df["vector"] = [[1., 1.,1.], [2., 2.,2.], [3., 3.,3.], [4., 4.,4.], [5., 5.,5.]]
df


index.upsert(vectors=zip(df.id, df.vector))
index.describe_index_stats()





romcom = ["Wall-E", "Ratatouille"]
romcom_df = df[df.id.isin(romcom)]
romcom_df


index.upsert(vectors=zip(romcom_df.id, romcom_df.vector), namespace="romcom")
index.describe_index_stats()





query_results = index.query(vector = df[df.id == "Wall-E"].vector[0], top_k = 3, namespace = "romcom")

query_results


pc.delete_index(index_name)





random_state = 42
np.random.seed(random_state)

sample_size = 1000
dim = 3
A_mean = 0.
B_mean = 2.

A_vectors = A_mean + np.random.randn(sample_size, dim) # centre it around mean
B_vectors = B_mean + np.random.randn(sample_size, dim) # centre it around mean


query_size = 20
A_queries = A_mean +np.random.randn(query_size, dim)





# define nice colours:
pink = (206 / 255, 0 / 255, 65 / 255)
blue = (0 / 255, 0 / 255, 163 / 255)
green = (24 / 255, 187 / 255, 169 / 255)

plot_df = pd.concat(
    [
       pd.DataFrame(dict(D1=A_vectors[:, 0], D2=A_vectors[:, 1], cluster="A")),
        pd.DataFrame(dict(D1=B_vectors[:, 0], D2=B_vectors[:, 1], cluster="B")),
        pd.DataFrame(dict(D1=A_queries[:, 0], D2=A_queries[:, 1], cluster="Queries")), 
    ]
)
plot_df


# render as static images inline with the code output
%matplotlib inline

fig, ax = plt.subplots(figsize=(8, 4))
ax.tick_params(labelsize=10)
ax.xaxis.label.set_size(12)
ax.yaxis.label.set_size(12)
sns.scatterplot(
    data=plot_df,
    x="D1",
    y="D2",
    hue="cluster",
    palette=[green, pink, blue],
    s=10,
    ax=ax,
)
ax.legend(
    title="cluster", loc="upper left", borderpad=0.2, fancybox=False, framealpha=0.5
)
plt.tight_layout()
plt.box(on=None)
plt.show()





index_name = "simple-knn-classifier"

create_index(index_name)
index = pc.Index(index_name)


items_df = pd.DataFrame()
items_df["id"] = [f"A-{ii}" for ii in range(len(A_vectors))] + [
    f"B-{ii}" for ii in range(len(B_vectors))
]
items_df["vector"] = [*A_vectors.tolist(), *B_vectors.tolist()]
items_df.sample(5)


items_df.shape


def chunks(iterable, batch_size = 100):
    it = iter(iterable)
    # break it into a chunk w batch size sample
    chunk = tuple(itertools.islice(it, batch_size))
    # repeat the process until chunk disappears
    while chunk:
        # suspend the function and return the value then continue
        yield chunk
        chunk = tuple(itertools.islice(it, batch_size))
        

# get the row iterations and create chunks inserted into the index
for vectors in chunks(((row.id, row.vector) for _, row in items_df.iterrows())):
    index.upsert(vectors = vectors)


# index indo
index.describe_index_stats()


for query_vec in A_queries.tolist():
    results = index.query(vector = query_vec, top_k = 10)
    cc = Counter(match.id.split("-")[0] for match in results.matches)
    print(f" 'Count nearest neighbours' class labels: A = {cc['A']} B = {cc['B']}")





pc.delete_index(index_name)
