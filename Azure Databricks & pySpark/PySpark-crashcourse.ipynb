{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2bd007-146c-4b31-ba61-4dc311824c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fa691-5461-4fda-bdbc-22ef8cabd85b",
   "metadata": {},
   "source": [
    "# **pySpark Introduction**\n",
    "\n",
    "This project uses `PySpark` which makes it a bit slower but still is adopted and used by developers (python developer community)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf21e5-403d-410c-96ca-933d4c7325b0",
   "metadata": {},
   "source": [
    "Spark distributes our data across multiple machines / cluster of machines improving data processing process. More detailed description about the cluster management system in Spark is referenced in [databricks and pySpark notes](databricks&pySpark-notes.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c0233-42f6-4e7b-a02d-305741b2b607",
   "metadata": {},
   "source": [
    "## Setting up the Spark environment\n",
    "\n",
    "First we need to import pyspark and use Sparkcontext which will create a local spark environment. We want to use our local machine for it and more instructions on how to set it up are present at [pyspark docs](https://spark.apache.org/docs/0.9.0/quick-start.html#a-standalone-app-in-python)\n",
    "\n",
    ">Only one SparkContext should be active per `JVM`. You must stop() the active SparkContext before creating a new one.\n",
    "> Make sure you have installed and linked openjdk and spark files and set up spark configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f209e9b-8ee2-4449-ac36-6255f5f77caf",
   "metadata": {},
   "source": [
    "You need to download the following things to download and make it work:\n",
    "- `JDK` environment\n",
    "- `Spark` analytics engine\n",
    "- `winlete` app for the `Hadoop` framework, *this may not be necessary but my local machine required it for it to work properly.*\n",
    "- `Python` and `jupyter notebook` -- primary language and the workspace\n",
    "\n",
    "Make sure to add all the elements above as `NAME_HOME` in you system environment variables and then set the path to their bin folders accordingly.\n",
    "For windows, it would be as following in the `system environment variable` app:\n",
    "\n",
    "<img src =\"images/pyspark-env.png\">\n",
    "\n",
    "- oneDrive is not necessary here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b3453-ebe0-4ad8-81cd-80b3b2dfea86",
   "metadata": {},
   "source": [
    "We will also need `findspark` because it adds psypark to sys.path at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcb5484-1d02-4510-b94d-4824a7df5f38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\spark-3.5.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark \n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476a3104-3f14-4992-b368-72f5a9e0751c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Creating a SparkConf object to configure the Spark application\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\")  # You can change master URL as needed\n",
    "\n",
    "# Creating a SparkContext object\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126d4af-42fc-4ba6-b844-9777a3a4b2a1",
   "metadata": {},
   "source": [
    "We have access to the Spark through the variable `sc` now and have to stop after our process with `stop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00db0538-a65d-4498-b593-319b06e3684e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://128.189.206.249:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MyApp>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b52a60d-5eb8-4f58-908b-ca0f4fb67b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sc.stop() #commented out because we still need it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7faa8b-1bdd-4741-a2cf-68f72f45688b",
   "metadata": {},
   "source": [
    "## Parallelising with Spark\n",
    "\n",
    "Now we will set up some data to be parallelised. This list of number on the `driver` which will tell the `worker nodes` what to do to process the data the via a `cluster manager`. More information in [databricks and pySpark notes](databricks&pySpark-notes.md).\n",
    "\n",
    "We are going to take a python list and distribute it into an `rdd`, `resilient distributed dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6247dc-047c-40fd-b6e3-91a866b5c59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = list(range(1,1_000001))\n",
    "\n",
    "nums_rdd = sc.parallelize(nums)\n",
    "nums_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3d854-c3f2-4872-b302-f3ea55a89e1e",
   "metadata": {},
   "source": [
    "We can ask for whats in the `rdd` with the `collect()` method. However, this is a risky operations as there can be a A LOT OF DATA distributed through different machines and we don't want to bring all that data back to the `driver` for no reason.\n",
    "\n",
    "\n",
    "We can `take(n)` to only look at the `n` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e7e01f-607b-4405-8119-af1c5d3a72c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " 1000,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b9c527-28c9-4041-8ac7-ade52cf5245a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4252045-642f-4feb-843f-f8424fb9a284",
   "metadata": {},
   "source": [
    "# **Data manipulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45ce76-7b60-4cc4-b00e-6d2c86159efe",
   "metadata": {},
   "source": [
    "To do any sort of manipulations to the data, we can `map()` a function to it, such as the following procedure which squares the number. We can do the same to have some composite functions such as forming a tuple of the squared number and the number of digits in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7013382-7ca2-4e90-ad00-cf69e36cc746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_nums_rdd = nums_rdd.map(lambda x: x**2)\n",
    "squared_nums_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73bb20f-916e-41f1-aef2-0bc11251319a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (4, 1), (9, 1), (16, 2), (25, 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = squared_nums_rdd.map(lambda x: (x, len(str(x))))\n",
    "pairs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3e655-e12a-49ee-98e4-57370b2f6a65",
   "metadata": {},
   "source": [
    "We can use the same syntax to `filter()` out the data with specific conditions. The following code will only retain the data with even number of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6c7c80-116f-4ee5-9770-e6dfd0fc1f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 2),\n",
       " (25, 2),\n",
       " (36, 2),\n",
       " (49, 2),\n",
       " (64, 2),\n",
       " (81, 2),\n",
       " (1024, 4),\n",
       " (1089, 4),\n",
       " (1156, 4),\n",
       " (1225, 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_filtered = pairs.filter(lambda x: (x[1] % 2) == 0)\n",
    "pairs_filtered.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b697d31-80bf-4908-ae64-6d1e7928c291",
   "metadata": {},
   "source": [
    "We can also group to and the summarise the data to get aggregrated information or some summary statistics. We will use `groupByKey()` so, we need to flip the pairs and we can the number of digits as the key. As the grouped items are iterable, we can treat them as list and then take the summary statistic fo them, mean, in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d46fd19-ffa2-4c49-8ca3-998a2a787363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, <pyspark.resultiterable.ResultIterable at 0x2038dcf41d0>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x2038dcf5950>),\n",
       " (10, <pyspark.resultiterable.ResultIterable at 0x2038dcf48d0>),\n",
       " (4, <pyspark.resultiterable.ResultIterable at 0x20388aeda10>),\n",
       " (12, <pyspark.resultiterable.ResultIterable at 0x2038dcf5a50>),\n",
       " (6, <pyspark.resultiterable.ResultIterable at 0x2038732d050>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flip the pairs\n",
    "flipped_pairs = pairs_filtered.map(lambda x: (x[1], x[0]))\n",
    "grouped = flipped_pairs.groupByKey() # takes a bit longer as grouping is a more complex function\n",
    "grouped.take(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d98cf29-2543-4a94-b77b-db2232551a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_list = grouped.map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e0028-12ea-4d6b-aab0-e74c89bde9e1",
   "metadata": {},
   "source": [
    "Not going to show the number as these lists can get pretty long, even the nested arrays will have large number of elements. However, I am doing the `list` typecasting and summary statistic calculations in 2 steps in order to be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61383c6f-2ee0-41df-aa70-bc6545127f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 47204941.666666664),\n",
       " (2, 45.166666666666664),\n",
       " (10, 4720705565.0),\n",
       " (4, 4675.5),\n",
       " (12, 472075391214.1667),\n",
       " (6, 471838.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged = grouped_list.map(lambda x: (x[0], sum(x[1])/len(x[1])))\n",
    "averaged.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00773af-d61d-4d4b-828f-cddb8845d786",
   "metadata": {},
   "source": [
    "As we can see that due the squared numbers, the mean values were quite high. Just to make it look nicer, we will also sort it in the correct order. As sorting is already a high complexity function in comparison to mapping, this will also take a long while, especially with `rdd`\n",
    "\n",
    "> In our case, the data was small enough to be stored in the local machine so we should have, stored there and then sorted. But either ways, try to minimise sorting processes in our data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b065f2d6-5cd7-4324-8058-09766066fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 45.166666666666664),\n",
       " (4, 4675.5),\n",
       " (6, 471838.0),\n",
       " (8, 47204941.666666664),\n",
       " (10, 4720705565.0),\n",
       " (12, 472075391214.1667)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged = averaged.sortByKey()\n",
    "averaged.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc09782-643a-4bcb-b08f-b742c9cc4fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7080ef-dceb-455b-b1b5-badb8af3bcb2",
   "metadata": {},
   "source": [
    "# **Dataframes in pySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd03788-6d5d-4e35-a3d3-0f82ea2015a2",
   "metadata": {},
   "source": [
    "We can also with dataframes with spark which will be the main purpose in data science projects. However, We will need to use ***SparkSession*** instead of ***SparkContext***. \n",
    "\n",
    "The main difference between the two:\n",
    "\n",
    " | SparkSession | SparkContext |\n",
    " | ------------ | ------------ |\n",
    " | used to access the underlying Spark environment | used to access the data stored in Spark  |\n",
    " | better for working data frames and datasets | better for working with RDD and low-level spark features |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cef9f7-232b-4afb-b3a8-e112121d8740",
   "metadata": {},
   "source": [
    "We are first going to create a SparkSession with same name as above and then create our data and schema to work with the dataframe. Similar to `sc.stop()`, we can stop the Session with `spark.stop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7027206-2fac-46ec-a5df-850d2b3b9124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "data = [('james',23,4000,'rude person'), \n",
    "        ('nupur',19,10_000,'the nicest person'), \n",
    "        ('paul',40,3000, 'goldilock'),\n",
    "       ('karen',40,2000,'karen')]\n",
    "\n",
    "\n",
    "cols = ['name','age','score','comments']\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = cols)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f28d5-e556-464c-978b-6c3cf402ec10",
   "metadata": {},
   "source": [
    "We can show our df using the `show()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9673a843-d1c1-4195-9e4f-cae59f1f811a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-----------------+\n",
      "| name|age|score|         comments|\n",
      "+-----+---+-----+-----------------+\n",
      "|james| 23| 4000|      rude person|\n",
      "|nupur| 19|10000|the nicest person|\n",
      "| paul| 40| 3000|        goldilock|\n",
      "|karen| 40| 2000|            karen|\n",
      "+-----+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382acd74-626c-408b-8f13-7a37608572f4",
   "metadata": {},
   "source": [
    "We can do the same sort of manipulations as before but now we have to provide the ey. Such as the following function does a simple `groupBy` and then aggregrate using the mean / `avg`. However, in such a case, the age will become the index. We can also sort by the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71c8341-752c-4041-b215-70f5f959cb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|age|avg(score)|\n",
      "+---+----------+\n",
      "| 19|   10000.0|\n",
      "| 23|    4000.0|\n",
      "| 40|    2500.0|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_df = df.groupBy(\"age\").agg({'score':'mean'})\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671fb72-6053-4fce-8797-1bfb3523b84d",
   "metadata": {},
   "source": [
    "This is nice but maybe we want other attributes back so we can `join()` the two tables no age as following. We will have to mention which columsn to `select()` to be specific and we will have to mention on what attribute to join and how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4ff421-c2d1-4a72-8020-dbfbfce067d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-----------------+\n",
      "|age|avg(score)| name|score|         comments|\n",
      "+---+----------+-----+-----+-----------------+\n",
      "| 19|   10000.0|nupur|10000|the nicest person|\n",
      "| 23|    4000.0|james| 4000|      rude person|\n",
      "| 40|    2500.0|karen| 2000|            karen|\n",
      "| 40|    2500.0| paul| 3000|        goldilock|\n",
      "+---+----------+-----+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_full = avg_df.join(df.select('name','age','score','comments'), on = 'age', how = 'left')\n",
    "avg_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cea50-02ef-49ae-8261-ae7c651a3bf0",
   "metadata": {},
   "source": [
    "We can also order them according to the comments if we want. We can simply mention `orderBy(attribute)` or sort it in a descending order by giving it the series and mentioning the `desc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b39bd7-d440-4db6-839d-b7e1ccff0c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-----------------+\n",
      "|age|avg(score)| name|score|         comments|\n",
      "+---+----------+-----+-----+-----------------+\n",
      "| 40|    2500.0| paul| 3000|        goldilock|\n",
      "| 40|    2500.0|karen| 2000|            karen|\n",
      "| 23|    4000.0|james| 4000|      rude person|\n",
      "| 19|   10000.0|nupur|10000|the nicest person|\n",
      "+---+----------+-----+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_full_sorted = avg_full.orderBy(avg_full['age'].desc())\n",
    "avg_full_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0425e68-5f1e-4abb-a6a9-860ab6493528",
   "metadata": {},
   "source": [
    "## Further notes\n",
    "\n",
    "We can also mention the data type in the schema using DDL:\n",
    "\n",
    "```python\n",
    "schema = \"name STRING, age INT, score INT, comments STRING\"\n",
    "```\n",
    "\n",
    "or define it programmatically:\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('age', IntegerType(), False),\n",
    "    StructField('score', IntegerType(), True),\n",
    "    StructField('comments', StringType(), False)\n",
    "])\n",
    "```\n",
    "\n",
    "### Dataset vs DataFrame\n",
    "\n",
    "There is also something called DataSet in Spark and the main differences are as following:\n",
    "\n",
    "| DataFrame | DataSet |\n",
    "| --------- | ------- | \n",
    "|  distributed collection of data organized into named columns | distributed collection of data that provides the benefits of RDDs, strong type, lambda funcs and Spark SQL's optimised engine |\n",
    "| similar to dataframes in Python and R | similar to SQL exec |\n",
    "| API avail in Python, Scala, Java, R | API available in Java and Scala | \n",
    "| high-level abstraction | compile time sagety |\n",
    "| optimised and suggested for more structured data | flexible for working with structured and unstructured data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0d972-3092-4053-b1f1-a0b15b0c9fcf",
   "metadata": {},
   "source": [
    "# **Machine Learning Libraries in Spark**\n",
    "\n",
    "We can use Apache Spark `Mlib` to create machine learning model which includes training a model, tuning a model and managing a model. I will skip some sections in this lecture as those notes will be similar to the concepts of creating an ML pipeline in `Sklearn` or `TensorFlow` with which I already have experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421e33e-0b8a-42d5-8e4d-6f039cd4e295",
   "metadata": {},
   "source": [
    "We will first download ML specific libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd3e625-e4b7-4f4b-9b50-b44b86647b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import RFormula\n",
    "import pandas as pd\n",
    "import click"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf130a6-bd9b-455f-884b-b2c2564a6377",
   "metadata": {},
   "source": [
    "We will create an ML model to predict price of an airbnb apartment. We will use a simpler LinearRegression model.\n",
    "\n",
    "Firstly, We read the data as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1229716-7e0f-4a91-b5ee-82ac27c03ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = \"ML_data.parquet\"\n",
    "\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "\"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872795ea-296a-4426-92fc-aa541fd272a0",
   "metadata": {},
   "source": [
    "Then we `randomSplit` the training and testing data, then we `vectorise` the predictors and create a pipeline to see how it performs against the test data. I will never test against the `testDF` in a real model building project but rather have a validation set or perform cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d21471-9c0a-43b6-bc70-84705d4c31ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set,\n",
      "and 1366 in the test set\n",
      "\n",
      "The formula for the linear regression line is\n",
      "price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set,\n",
    "and {testDF.count()} in the test set\"\"\")\n",
    "\n",
    "# take bedrooms and vectorise it to make features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)\n",
    "\n",
    "# the model parameters\n",
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "\n",
    "print(f\"\"\"\n",
    "The formula for the linear regression line is\n",
    "price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91c2d5-04f6-4f2f-ae77-2d680ac58d7f",
   "metadata": {},
   "source": [
    "We will not create a simple pipeline using the two steps above of using a `VecAssembler` and `LR` fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c06d84f-127a-475e-b2a8-1f00f6355d20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "# predicted data\n",
    "predDF_simple = pipelineModel.transform(testDF)\n",
    "predDF_simple.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4200901-4b47-454e-a7b3-1495442ba34d",
   "metadata": {},
   "source": [
    "Now we will use all the features and use different data manipulations on them.\n",
    "\n",
    "1. Check for columns that are of string type and store them separately\n",
    "2. String columns will have`StringIndexing` and `OHE` performed on them and add them and added into our models.\n",
    "3. Extract double values as double (except for price)\n",
    "4. vectorise all of them\n",
    "5. Create a DataFrame of predictors using the same syntax as in `R`\n",
    "6. Create the same pipeline as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "779e0c56-2aff-4798-851b-ddba2c3fc281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "\n",
    "# string input\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "\n",
    "# Assemble features\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "rFormula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63bc6d30-4416-4c7d-8994-0df93ed32a71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,7,23,4...| 85.0| 55.30094763354373|\n",
      "|(98,[0,3,6,7,23,4...| 45.0| 22.70940291742818|\n",
      "|(98,[0,3,6,7,23,4...| 70.0|27.182906571761578|\n",
      "|(98,[0,3,6,7,13,4...|128.0|-91.90969569190747|\n",
      "|(98,[0,3,6,7,13,4...|159.0| 94.54162775821169|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "# Or use RFormula\n",
    "pipeline = Pipeline(stages = [rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6de501b4-e72a-4704-8711-e02506ed9175",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [-10.680348908293674,-1390.952489733411,-1402.527978674161,-1386.907903065293,-1454.4484899169297,-1402.116377384968,-6.362146769272623,-0.056621067972006604,55.89424123365027,38.80825515478183,8.383043179873187,6.019001031007852,50.84662599297539,29.67477717334662,18.586989107252542,59.96268328414251,39.97943885288819,19.123807416768702,6.715022797603014,-22.838679671928915,11.96369190276733,39.26948325037505,14.28798691554543,-9.579600274544747,49.53153102087754,72.8069233455944,-41.2218298793475,-4.5342329044995635,105.76724472030375,-3.021675321475595,131.87149838117,65.28472932889453,-42.4603941265768,95.4065720527332,39.15524906601873,105.83164117979906,44.611544569888224,84.95910377737069,85.88653390835726,81.955903142468,222.7141012973797,48.10397820489529,64.26407019205433,2486.8385272461405,789.8432072531353,-37.71597343844105,1.356268599951937,28.980391021534427,-20.437568214523672,4.234126737275918,-16.177839312194575,-6.751182988707704,-35.18785362408661,35.10330482377562,-68.85701202766519,-98.94137523999876,1.0538289705016621,-22.635074666146814,-10.124176540506904,-7.197230326575998,36.100248721143636,-4.285001547045334,108.34853863529915,-97.21259592441915,-15.803723020899296,-109.65874622224914,-82.09844812894649,68.41980838967561,425.1927282066555,104.30337532649098,50.40142709014465,27.7422342957064,19.70107850038361,59.52715527540469,1.4088013333322458,-22.62718580077554,-13.855248723183987,-40.23677809820275,-15.298153873905566,-1.2959712427016805,-0.18927012771239585,1.5903410218451115,-1.0879007459339183,24.76083048807379,0.8016598666149732,-7.023764946476451,8.335822051461312,-25.541794488696517,-54.94620909995906,-9.73260909905846,-6.622582211872791,61.94400526655503,57.51951084693084,63.59676148288758,-11.160593131891059,-128.37720307014038,-11.16059313193796,-11.16059313193796]\n",
      "Model intercept: 3995.479074594956\n"
     ]
    }
   ],
   "source": [
    "# # Show the model summary\n",
    "print(\"Model coefficients: \" + str(model.stages[-1].coefficients))\n",
    "print(\"Model intercept: \" + str(model.stages[-1].intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb2e39-c42b-431a-9435-f893f2842b2c",
   "metadata": {},
   "source": [
    "We are going to evaluate this model using $R^2$ and $RMSE$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a44ebca0-5ce4-4dbf-96b0-c29addadbaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 221.6\n",
      "R2 is 0.15985154393435386\n"
     ]
    }
   ],
   "source": [
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol = \"prediction\",\n",
    "labelCol = \"price\",\n",
    "metricName = \"rmse\")\n",
    "\n",
    "rmse = regressionEvaluator.evaluate(predDF_simple)\n",
    "print(f\"RMSE is {rmse:.1f}\")\n",
    "\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
